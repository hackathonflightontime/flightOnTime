# âœˆï¸ MicroserviÃ§o de PrevisÃ£o â€” flight-on-time

MicroserviÃ§o FastAPI responsÃ¡vel por:

* carregar o modelo `.joblib` treinado
* receber dados de voo via POST
* processar features
* gerar previsÃ£o (0 = Pontual, 1 = Atrasado)
* retornar probabilidade

O backend Java consultarÃ¡ **esse serviÃ§o** para fazer previsÃµes.

---

# ğŸ“˜ 1. Estrutura do MicroserviÃ§o

```
service/
â”‚â”€â”€ app.py                 â†’ cÃ³digo principal FastAPI
â”‚â”€â”€ requirements.txt       â†’ dependÃªncias Python
â”‚â”€â”€ modelo_voo.joblib      â†’ arquivo exportado (gerado apÃ³s treinamento)
â”‚â”€â”€ README_MICROSERVICE.md â†’ este documento
```

---

# ğŸ”§ 2. Como Rodar Localmente

### Passo 1 â€” Instalar dependÃªncias

```
pip install -r requirements.txt
```

### Passo 2 â€” Rodar o servidor

```
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

### Acessar documentaÃ§Ã£o automÃ¡tica:

```
http://localhost:8000/docs
```

---

# ğŸ§  3. Estrutura Base do `app.py`

```python
from fastapi import FastAPI
import joblib
import pandas as pd

app = FastAPI()

# Carregamento do modelo (ajustado apÃ³s treinamento)
# modelo = joblib.load("../model/modelo_voo.joblib")

@app.get("/")
def root():
    return {"status": "microserviÃ§o funcionando"}

@app.post("/predict-model")
def predict(data: dict):
    """
    data esperado:
    {
      "companhia": "AZ",
      "origem": "GIG",
      "destino": "GRU",
      "data_partida": "2025-11-10T14:30:00",
      "distancia_km": 350
    }
    """

    # TODO: converter entrada em features corretas
    # exemplo_x = converter_para_features(data)

    # TODO: usar modelo
    # pred = modelo.predict([exemplo_x])[0]
    # prob = modelo.predict_proba([exemplo_x])[0][1]

    return {
        "previsao": "pendente",
        "probabilidade": 0.0
    }
```

Depois vamos substituir os **TODOs** pelo processamento real.

---

# ğŸ§© 4. Formato esperado da entrada

O backend Java envia:

```json
{
  "companhia": "AZ",
  "origem": "GIG",
  "destino": "GRU",
  "data_partida": "2025-11-10T14:30:00",
  "distancia_km": 350
}
```

---

# ğŸ§ª 5. Formato esperado da resposta

```json
{
  "previsao": "Atrasado",
  "probabilidade": 0.78
}
```

---

# ğŸ”Œ 6. IntegraÃ§Ã£o com o Backend Java

O backend chamarÃ¡ o endpoint via HTTP:

```
POST http://localhost:8000/predict-model
```

Ou, no ambiente de produÃ§Ã£o (OCI):

```
POST http://IP_DA_VM_OCI:8000/predict-model
```

---

# ğŸ³ 7. Rodando com Docker

O microserviÃ§o serÃ¡ incluÃ­do no `docker-compose.yml`:

```yaml
microservice:
  build: ./datascience/service
  container_name: flight_microservice
  ports:
    - "8000:8000"
```

Exemplo de Dockerfile:

```dockerfile
FROM python:3.10

WORKDIR /app
COPY . .
RUN pip install --no-cache-dir -r requirements.txt

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

# â˜ï¸ 8. Deploy na OCI (resumo)

1. Criar VM
2. Instalar Docker + Docker Compose
3. Copiar o microserviÃ§o para a VM
4. Rodar:

```
docker-compose up -d microservice
```

5. Liberar porta 8000 no firewall da OCI

---

# âœ”ï¸ 9. Checklist para produÃ§Ã£o

* modelo `.joblib` estÃ¡ dentro da pasta `model`
* app.py faz o carregamento correto
* conversÃ£o de features estÃ¡ implementada
* porta estÃ¡ liberada
* docker-compose configurado

---

# ğŸ‘¨â€ğŸ’» 10. ResponsÃ¡vel

Time de Data Science / MLOps
Acompanhamento: **Darlei**
